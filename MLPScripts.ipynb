{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebb4297",
   "metadata": {},
   "source": [
    "# ðŸ§  MPL Trainer Examples\n",
    "Three Python Scripts that train MLP Models with different data types, functions and results.\n",
    "\n",
    "Scripts:\n",
    "- MLP XOR Model Trainer.\n",
    "- MLP Circle Dots Model Trainer.\n",
    "- MLP Text Classifier Trainer.\n",
    "\n",
    "## ðŸ”€ MLP XOR Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR Dataset\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# MLP Model Parameters\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # 2 â†’ 4 hidden neurons\n",
    "        self.output = nn.Linear(4, 1)  # 4 â†’ 1 output neuron\n",
    "        self.relu = nn.ReLU()  # ReLU activation for hidden layer\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))  # Hidden layer with ReLU\n",
    "        x = self.sigmoid(self.output(x))  # Output layer with Sigmoid\n",
    "        return x\n",
    "\n",
    "# Model Training\n",
    "model = MLP()\n",
    "loss_function = nn.BCELoss()  # Binary Cross Entropy Loss \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)  # Learning rate\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_function(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test Trained Model\n",
    "print(\"\\nTrained Outputs:\")\n",
    "predictions = model(X).detach()\n",
    "rounded_predictions = torch.round(predictions)  # Round the predictions to 0 or 1\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = (rounded_predictions.eq(Y)).sum()\n",
    "accuracy = correct_predictions / Y.size(0) * 100\n",
    "print(f\"Accuracy: {accuracy.item()}%\")\n",
    "\n",
    "print(rounded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ef39f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ MLP Circle Dots Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "def generate_circle_data(num_points=100):\n",
    "    # Generate random data points\n",
    "    X = np.random.rand(num_points, 2)  # Random points in a 2D space\n",
    "    y = np.array([1 if (x[0]**2 + x[1]**2 > 0.25) else 0 for x in X])  # Inside circle is 0, outside is 1\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_train, Y_train = generate_circle_data(1000)  # Generate 1000 data points\n",
    "X_test, Y_test = generate_circle_data(200)  # Generate 200 test points\n",
    "\n",
    "# MLP Model Parameters\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Model Training\n",
    "model = MLP()\n",
    "loss_function = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) # Learning rate\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_function(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test Trained Model\n",
    "with torch.no_grad():\n",
    "    Y_pred = model(X_test).round()  # Round predictions to get 0 or 1\n",
    "    accuracy = (Y_pred == Y_test).sum().item() / Y_test.size(0)\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    inside_count = (Y_pred == 0).sum().item()  # Count for points inside the circle (label 0)\n",
    "    outside_count = (Y_pred == 1).sum().item()  # Count for points outside the circle (label 1)\n",
    "\n",
    "    print(f\"Points inside the circle: {inside_count}\")\n",
    "    print(f\"Points outside the circle: {outside_count}\")\n",
    "\n",
    "# Plot the Data (Visualization)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_pred.squeeze(), cmap='coolwarm', marker='o', alpha=0.6)\n",
    "plt.title(\"Pattern Classification (Circle vs Outside)\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d50e8",
   "metadata": {},
   "source": [
    "## ðŸ”  MLP Text Classifier Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset\n",
    "word_to_vec = {\n",
    "    'apple': [1, 0, 0, 0],    # Elements of Group 0 (7)\n",
    "    'banana': [0, 1, 0, 0],\n",
    "    'cherry': [0, 0, 1, 0],\n",
    "    'date': [0, 0, 0, 1],\n",
    "    'orange': [1, 0, 0, 0],\n",
    "    'grape': [0, 1, 0, 0],\n",
    "    'pear': [0, 0, 1, 0],\n",
    "    'dog': [0, 0, 0, 1],      # Elements of Group 1 (7)\n",
    "    'car': [0, 0, 0, 1],\n",
    "    'phone': [0, 0, 1, 0],\n",
    "    'train': [1, 0, 0, 0],\n",
    "    'ball': [0, 1, 0, 0],\n",
    "    'computer': [1, 0, 0, 0],\n",
    "    'table': [0, 1, 0, 0],\n",
    "}\n",
    "\n",
    "# Label Elements with their groups\n",
    "X = torch.tensor(list(word_to_vec.values()), dtype=torch.float32)  # One-hot encoded vectors\n",
    "Y = torch.tensor([[0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1]], dtype=torch.long)  # Labels for CrossEntropyLoss\n",
    "\n",
    "# MLP Model Parameters\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(4, 8)  # Layers 4 input, 8 hidden\n",
    "        self.hidden2 = nn.Linear(8, 16) # Added second hidden layer\n",
    "        self.output = nn.Linear(16, 2)  # Output 2 units (binary)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))  # Pass through second hidden layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Model Training\n",
    "model = MLP()\n",
    "loss_function = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_function(outputs, Y.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        predictions = torch.argmax(outputs, dim=1).tolist()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}, Predictions: {predictions}\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "with torch.no_grad():\n",
    "    outputs = model(X)\n",
    "    predicted_classes = torch.argmax(outputs, dim=1)\n",
    "    correct = (predicted_classes == Y.squeeze()).sum().item()\n",
    "    accuracy = correct / len(Y) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Test Trained Model\n",
    "def classify_word(word):\n",
    "    if word in word_to_vec:\n",
    "        vec = torch.tensor(word_to_vec[word], dtype=torch.float32).view(1, -1)\n",
    "        with torch.no_grad():\n",
    "            output = model(vec)\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "            print(f\"Word: {word}, Group: {predicted_class}\")\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found in the dictionary.\")\n",
    "\n",
    "# Example Usage\n",
    "classify_word(\"apple\")   # Should classify as group 0\n",
    "classify_word(\"dog\")     # Should classify as group 1\n",
    "classify_word(\"car\")     # Should classify as group 1\n",
    "classify_word(\"cherry\")  # Should classify as group 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
